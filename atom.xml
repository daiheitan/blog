<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zimon Dai&#39;s Random Thoughts</title>
  
  
  <link href="/blog/atom.xml" rel="self"/>
  
  <link href="http://daiheitan.github.io/blog/"/>
  <updated>2019-04-25T10:00:47.122Z</updated>
  <id>http://daiheitan.github.io/blog/</id>
  
  <author>
    <name>Zimon Dai</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>RustCon Asia 2019 Talk</title>
    <link href="http://daiheitan.github.io/blog/2019/04/25/RustCon-Asia-2019-Talk/"/>
    <id>http://daiheitan.github.io/blog/2019/04/25/RustCon-Asia-2019-Talk/</id>
    <published>2019-04-25T09:23:23.000Z</published>
    <updated>2019-04-25T10:00:47.122Z</updated>
    
    <content type="html"><![CDATA[<p>On April 20th, I was invited to give a talk on <em>RustCon Asia 2019</em>, a fantastic local event held by <em>Pingcap</em> and <em>Cryptape</em>.</p><p>The talk is about a distributed actor system I have been developing and using for nearly half a year, codenamed as <em>UPS</em>. We know that <code>actix</code> is the most famous and successful actor system in Rust ecosystem. UPS is different as it is a distribution-first solution. It picks fast codes for message se/de, integrates with <code>tokio</code> for networking. For each runtime worker, the actors are stored as plain objects, following the <a href="https://en.wikipedia.org/wiki/Entity_component_system" target="_blank" rel="noopener">ECS pattern</a>, which means UPS allows more than one actor per-type.</p><p>UPS is developed for large work loads, distributed and streaming computation. I am planning to opensource this crate this year, under the name of my company, <em>Alibaba inc.</em></p><p>Enough intro, back to the talk. It’s about some problems I encountered when making this crate. Specifically, the talk is about 3 problems:</p><ul><li>Get compilation-stable <code>TypeId</code>, quite hacky</li><li>Use <em>specialization</em> for different codecs</li><li>The tick based design in UPS</li></ul><div class="row">    <embed src="/blog/distributed_actor_system_in_rust.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;On April 20th, I was invited to give a talk on &lt;em&gt;RustCon Asia 2019&lt;/em&gt;, a fantastic local event held by &lt;em&gt;Pingcap&lt;/em&gt; and &lt;em&gt;Crypt
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Data race, atomic and reordering</title>
    <link href="http://daiheitan.github.io/blog/2017/05/19/Data-race-atomic-and-reordering/"/>
    <id>http://daiheitan.github.io/blog/2017/05/19/Data-race-atomic-and-reordering/</id>
    <published>2017-05-19T02:44:17.000Z</published>
    <updated>2019-04-25T09:21:38.938Z</updated>
    
    <content type="html"><![CDATA[<p>Everything is much more complicated in a multithreaded world. Everyone knows about data races, while few knows why and how to effeciently avoid that.</p><p>Data race occurs when multiple threads accessing the same memory location at the same time (Read &amp; Write). If one thread writes to the memory, another thread is not updated and uses the old value to do its own calculcation, undefined behaviour happens. Data race should not be a problem is any interactions made to a memory location is well defined.</p><p>By using the word <em>interaction</em>, we mean <em>Read</em> and <em>Write</em>, or in the memory model terminology, <em>Load</em> and <em>Store</em>.</p><p>By using the word <em>well defined</em>, we mean the interactions for one memory model in one thread do not collision with other threads in this time period.</p><p>This post introduces memory reordering, how it affects multithreaded program behaviour, why does it happen and what we could do to avoid data race by avoiding it.</p><h1 id="Memory-Reordering"><a href="#Memory-Reordering" class="headerlink" title="Memory Reordering"></a>Memory Reordering</h1><p>Memory reordering is quite interesting. If writing single threaded program, you may never noticed this. As the key principle of memory reordering is keep single-threaded behaviour not changed. Reordering includes compiler and runtime reordering.</p><h2 id="Compile-time-reordering"><a href="#Compile-time-reordering" class="headerlink" title="Compile time reordering"></a><a href="http://preshing.com/20120625/memory-ordering-at-compile-time/" target="_blank" rel="noopener">Compile time reordering</a></h2><p>As the title says, this happens when the compiler thransforming your code into assembly code. The compiler trys to gather all IO to a memory location into one block to optimise memory usage.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> A, B;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    A = B + <span class="number">1</span>;</span><br><span class="line">    B = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>will be translated to:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">mov     eax, DWORD PTR B</span><br><span class="line">mov     DWORD PTR B, 0</span><br><span class="line">add     eax, 1</span><br><span class="line">mov     DWORD PTR A, eax</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>The <em>save</em> to <code>B</code> is lifted before the <em>save</em> to <code>A</code>, so I/O to <code>B</code> happens together. As mentioned before, this will not affect the runtime in a single-threaded program. But when it comes to multithreaded, the program may run not like what you expected.</p><p>To avoid compile time reordering, one could add compiler instructions to tell the compiler that reordering should not be done at certain position. In C++11, you could use <code>asm volatile(&quot;&quot; ::: &quot;memory&quot;)</code>. In rust you could use <code>asm!(&quot;&quot; ::: &quot;memory&quot; : &quot;volatile&quot;)</code>.</p><h2 id="Runtime-reordering"><a href="#Runtime-reordering" class="headerlink" title="Runtime reordering"></a>Runtime reordering</h2><p>This is the interecting part. The processor could do runtime reordering when executing your program. That’s because every process has a inline-cache (like 32KB size) which supported I/O in 1 or 2 cycles. But a trip to the memory could cost hundred of cycles. So the processor would like to cache as much as he could and visit the memory as less. Not only the read is a problem, when some data is written, it first comes to the L1 cache, then got flushed to the memroy and L2 cache when the L1 cache is full. We could say that the time point of I/O is totally undefined.</p><p>Let’s say we have 4 threads A, B, C and D. A and B both read then write to <code>variable</code>, C and D are <em>observers</em>. Despite of reordering that could occur on every thread, C and D may end with totally different result of IO observation. C may see Load-A -&gt; Load-B -&gt; Store-A -&gt; Store-B, whilc D may see Load-A -&gt; Store-A -&gt; Load-B -&gt; Store-B.</p><h3 id="Sequential-Consistency"><a href="#Sequential-Consistency" class="headerlink" title="Sequential Consistency"></a>Sequential Consistency</h3><p>If a system could keep the observation result same across every thread, we say this system has <em>Sequential Consistency</em>. In a sequential consist system, all I/O are in order, no extra worry.</p><h3 id="Relaxed-consistency"><a href="#Relaxed-consistency" class="headerlink" title="Relaxed consistency"></a>Relaxed consistency</h3><p>Note that the most popular system X86 (intel, AMD) is not sequential consist.This is the major problem when writing multithreaded programs. And that’s where memory barriers come in. The creator of java compiler, Doug Lea has a <a href="http://g.oswego.edu/dl/jmm/cookbook.html" target="_blank" rel="noopener">famous article</a> about compilers, in which he defined 4 types of memory barriers: LoadLoad, LoadStore, StoreStore, StoreLoad. E.g, <code>LoadStore</code> is to make a barrier between a <code>Load</code> and a following <code>Store</code>, to prevent the <code>store</code> to be reordered to happen before the <code>load</code>.</p><p>Relaxed consitency means that some reodering are allowed if they do not across the mentioned four types of barriers.</p><p>Major ref of the following is <a href="http://preshing.com/20120710/memory-barriers-are-like-source-control-operations/" target="_blank" rel="noopener">this</a></p><h4 id="LoadLoad"><a href="#LoadLoad" class="headerlink" title="LoadLoad"></a><code>LoadLoad</code></h4><p><code>LoadLoad</code> acts quite like a <code>git pull</code>. It meas that <em>a read A could only happens when a former read B happens</em>. Note that the “former B” read may or may not be the latest value of B, this read and the former read may even not be the same variable / memory location. It’s quite useful even though looks weak at the first glance.</p><p>Say we have a variable which is manipulated by several threads. We could create a second shared boolean variable <code>isChanged</code>, if <code>isChanged</code> is set to <code>true</code>, we know the variable is changed by another thread. If we read the variable <strong>at this time point</strong>, the value should be updated.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (isUpdated) &#123;</span><br><span class="line">    LOADLOAD_FENCE();</span><br><span class="line">    <span class="keyword">return</span> Value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The fact that <code>isUpdated</code> is the latest value or not is really not important. We now have a defined bound to between <code>isUpdated</code> and <code>Value</code>, that’s enough. This pattern is a basis to the widely used double check lock pattern.</p><h4 id="StoreStore"><a href="#StoreStore" class="headerlink" title="StoreStore"></a><code>StoreStore</code></h4><p>It acts quites like a <code>git push</code>. Using the previous example, a <code>StoreStore</code> barrier is useful when we need to update <code>Value</code></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Value = x;</span><br><span class="line">STORESTORE_FENCE();</span><br><span class="line">isUpdated = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>This ensures <code>isUpdated</code> is set <strong>after</strong> Value, and tightly bind the two stores.</p><h4 id="LoadStore"><a href="#LoadStore" class="headerlink" title="LoadStore"></a><code>LoadStore</code></h4><p>LoadStore barriers are needed only on those out-of-order procesors in which waiting store instructions can bypass loads.</p><h4 id="StoreLoad"><a href="#StoreLoad" class="headerlink" title="StoreLoad"></a><code>StoreLoad</code></h4><p><code>StoreLoad</code> is quite special here. It’s a strong memory barrier and more expensive, which ensures that <strong>ALL</strong> stores performed before the barrier are visible to other processors.</p><p>Note that this is not equal to a <code>LoadLoad</code> and a <code>StoreStore</code>. As <code>LoadLoad</code> may not get the latest version between all processors for you, but a <code>StoreLoad</code> makes sure of that.</p><h1 id="Prevent-data-race-with-a-lock"><a href="#Prevent-data-race-with-a-lock" class="headerlink" title="Prevent data race with a lock"></a>Prevent data race with a lock</h1><h2 id="Lock-basics"><a href="#Lock-basics" class="headerlink" title="Lock basics"></a>Lock basics</h2><p><a href="https://en.wikipedia.org/wiki/Lock_(computer_science" target="_blank" rel="noopener">Lock</a>) is designed to enforce mutual exclusion concurrency control. And it’s sync. You could prevent data race by acquire a lock, make your updates inside the scope, and finally release the lock.</p><p>Usually when we say <em>lock</em>, it refers to a <em>Mutex</em> lock(readwrite lock), which a lock must be held both reading and writing. There’s also RwLock allowing multiple readers <strong>or</strong> one writer. But in fact on many OS, rwlocks are just mutex locks underneath.</p><h3 id="Double-check-lock"><a href="#Double-check-lock" class="headerlink" title="Double check lock"></a>Double check lock</h3><p>Mutex lock is easy to understand and use, but indeed not so efficient when thought through. There’s double-check lock which works with memory barriers, explained <a href="http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/" target="_blank" rel="noopener">here</a></p><h1 id="Prevent-data-race-with-memory-fences"><a href="#Prevent-data-race-with-memory-fences" class="headerlink" title="Prevent data race with memory fences"></a>Prevent data race with memory fences</h1><p>If you <a href="http://preshing.com/20120612/an-introduction-to-lock-free-programming/" target="_blank" rel="noopener">do not want to use</a> locks, and hates atomics because of lacking of scale and performace in certain conditions, the choice left is to dive into the dirty ground of memory barriers.</p><h2 id="Memory-fences-Acquire-and-release"><a href="#Memory-fences-Acquire-and-release" class="headerlink" title="Memory fences: Acquire and release"></a>Memory fences: Acquire and release</h2><p>C++ 11 introduces low-level lock-free operations: acquire and release.</p><p>An <em>acquire fence</em> prevents memory reordering after a read. Which equals to a LoadLoad + LoadStore.</p><p>A <em>release fence</em> prevents memory reordering of write after read/write, equals to LoadStore + StoreStore</p><p>A well-told article <a href="http://preshing.com/20130922/acquire-and-release-fences/" target="_blank" rel="noopener">here</a>.</p><h1 id="Prevent-data-race-with-atomics"><a href="#Prevent-data-race-with-atomics" class="headerlink" title="Prevent data race with atomics"></a>Prevent data race with atomics</h1><p>Locks are heavy and inefficient. Luckily they are not the only option to avoid data races in multithreaded programming. As described in previous sections, if we could instruct the processors to load/store data in a proper way, data race could totally be avoided. This is lock-free programming.</p><h2 id="Force-SC-Java-volatile-and-C-Atomic"><a href="#Force-SC-Java-volatile-and-C-Atomic" class="headerlink" title="Force SC: Java volatile and C++ Atomic"></a>Force SC: Java volatile and C++ Atomic</h2><p>As mentioned before, in sequential consistency, IO are in-order. So if we could achieve SC, problem solved! This is how Java’s <code>volatile</code> is <a href="https://bartoszmilewski.com/2008/11/11/who-ordered-sequential-consistency/" target="_blank" rel="noopener">done</a> (also C++11’s atomic).</p><ul><li>Issue a StoreStore barrier before each volatile store.</li><li>Issue a StoreLoad barrier after each volatile store.</li><li>Issue LoadLoad and LoadStore barriers after each volatile load.</li></ul><p>After that, Java compiler will use data analysis to remove some of the locks. And if it’s X86 system, LoadLoad and StoreStore locks are translated to no-ops. That’s <a href="https://bartoszmilewski.com/2008/11/05/who-ordered-memory-fences-on-an-x86/" target="_blank" rel="noopener">because of</a> the special structure of X86. So there would be generally to say, only the <code>StoreLoad</code> barrier which is still quite heavy.</p><p>Atomics are really easy to use, with the <code>.load()</code> and <code>.store()</code> API we mentioned before. But the problem is only super basic data structures (like int32, boolean) has atomic types. Many widely used data structures do not have a standard atomic type.</p><p>Note that in C++ 11, Atomics are <strong>defaultly</strong> SC, you could still specify the memory ordering for every <code>load</code> and <code>store</code>. It’s explained <a href="https://bartoszmilewski.com/2008/12/01/c-atomics-and-memory-ordering/" target="_blank" rel="noopener">here</a></p><h2 id="Write-your-own-Atomic-type"><a href="#Write-your-own-Atomic-type" class="headerlink" title="Write your own Atomic type"></a>Write your own Atomic type</h2><p><code>load</code> and <code>store</code> are one-dimensional operations. It is not really enough for lock-free operations. Take the add operation as an example, you need to first <em>load</em> the value, perform the addition, and then <em>store</em> the new value back. However, all the different kinds of atomic operations could be built with one essential operation: Compare-and-Swap (CAS). C++11 defines CAS as:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shared.compare_exchange_weak(T&amp; oldValue, T newValue, ...);</span><br></pre></td></tr></table></figure><p>This API updates an atomic <code>shared</code> with <code>newValue</code>, only if current value stored in <code>shared</code> equals to <code>oldValue</code>, or it will not perform the update, instead just pull the stored value into <code>oldValue</code>.</p><p>A spinlock is easy to come up with:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">uint32_t</span> fetch_multiply(<span class="built_in">std</span>::atomic&lt;<span class="keyword">uint32_t</span>&gt;&amp; shared, <span class="keyword">uint32_t</span> multiplier)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">uint32_t</span> oldValue = shared.load();</span><br><span class="line">    <span class="keyword">while</span> (!shared.compare_exchange_weak(oldValue, oldValue * multiplier)) &#123; &#125;</span><br><span class="line">    <span class="keyword">return</span> oldValue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>By using this, you could write any Read-Modify-Write operations for custom atomic types.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Everything is much more complicated in a multithreaded world. Everyone knows about data races, while few knows why and how to effeciently
      
    
    </summary>
    
    
      <category term="Memory" scheme="http://daiheitan.github.io/blog/tags/Memory/"/>
    
      <category term="Concurrency" scheme="http://daiheitan.github.io/blog/tags/Concurrency/"/>
    
      <category term="Atomic" scheme="http://daiheitan.github.io/blog/tags/Atomic/"/>
    
  </entry>
  
  <entry>
    <title>setjmp/longjmp in Rust</title>
    <link href="http://daiheitan.github.io/blog/2016/12/21/setjmp-longjmp-in-Rust/"/>
    <id>http://daiheitan.github.io/blog/2016/12/21/setjmp-longjmp-in-Rust/</id>
    <published>2016-12-21T08:45:26.000Z</published>
    <updated>2019-04-25T09:21:38.937Z</updated>
    
    <content type="html"><![CDATA[<p><code>setjmp</code>/<code>longjmp</code> is used for <a href="https://en.wikipedia.org/wiki/Setjmp.h" target="_blank" rel="noopener">no-local jumps</a>, which means to ‘jump’ between different functions.</p><p><code>setjmp</code> is used to declare a place (which is identified by an integer id) to jump to, while <code>longjmp</code> actually jumps to a place with a <code>value</code>. Generally if <code>setjmp</code> returns an <code>value</code>, it means that a no-local jump has occurred at this place.</p><p>This technique is generally used for exception handling, it allows the program to kind of ‘rollback’ through the stack to some point the exception not has happened. Complex and dangerous (unexpected behaviour &amp; memory leaks) as these functions are, they are avoided mostly in modern code bases.</p><p>Recently I am using <a href="https://github.com/libjpeg-turbo/libjpeg-turbo" target="_blank" rel="noopener">libjpeg-turbo</a> bindings in Rust, which uses this technique for error handling. I’d like to share some experiences about dealing with libjpeg in Rust.</p><h2 id="Error-handling-in-Libjpeg"><a href="#Error-handling-in-Libjpeg" class="headerlink" title="Error handling in Libjpeg"></a>Error handling in Libjpeg</h2><p>Libjpeg uses a <code>jpeg_err_mgr</code> struct for error handling related stuff. The struct contains error messages, code, stack and a pointer named <code>error_exit</code> to a callback function. You could call <code>jpeg_std_err()</code> to initialize the struct.</p><p>If you do not assign a function to <code>error_exit</code>, libjpeg will terminate the process when error occurs. In Rust, this means the whole thread will shut down. You could use the <a href="https://doc.rust-lang.org/std/panic/fn.catch_unwind.html_" target="_blank" rel="noopener">catch_unwind</a> feature which is added to stable channel recently.</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">read_jpeg</span></span>(input_buffer: &amp;[<span class="built_in">u8</span>], target_width: <span class="built_in">u32</span>, target_height: <span class="built_in">u32</span>) -&gt; stdio::<span class="built_in">Result</span>&lt;<span class="built_in">Vec</span>&lt;<span class="built_in">u8</span>&gt;&gt; &#123;</span><br><span class="line">catch_unwind(AssertUnwindSafe(|| &#123;</span><br><span class="line"><span class="keyword">unsafe</span> &#123;</span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut</span> dinfo: jpeg_decompress_struct = mem::zeroed();</span><br><span class="line"><span class="keyword">let</span> size = mem::size_of_val(&amp;dinfo) <span class="keyword">as</span> size_t;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut</span> err: jpeg_error_mgr = mem::zeroed();</span><br><span class="line">dinfo.common.err = jpeg_std_error(&amp;<span class="keyword">mut</span> err);</span><br><span class="line">err.error_exit = <span class="literal">Some</span>(libjpeg_error_handler);</span><br><span class="line">jpeg_CreateDecompress(&amp;<span class="keyword">mut</span> dinfo, JPEG_LIB_VERSION, size);</span><br><span class="line"></span><br><span class="line">jpeg_mem_src(&amp;<span class="keyword">mut</span> dinfo, input_buffer.as_ptr(), input_buffer.len() <span class="keyword">as</span> <span class="built_in">u64</span>);</span><br><span class="line">jpeg_read_header(&amp;<span class="keyword">mut</span> dinfo, <span class="literal">true</span> <span class="keyword">as</span> <span class="built_in">i32</span>);</span><br><span class="line"><span class="keyword">let</span> image_width = dinfo.image_width;</span><br><span class="line"><span class="keyword">let</span> image_height = dinfo.image_height;</span><br><span class="line"><span class="keyword">if</span> target_width != image_width || target_height != image_height &#123;</span><br><span class="line"><span class="keyword">let</span> (scale, scale_denom) = select_decompress_idct_factor(image_width, image_height, target_width, target_height);</span><br><span class="line">dinfo.scale_num = scale;</span><br><span class="line">dinfo.scale_denom = scale_denom;</span><br><span class="line">&#125;</span><br><span class="line">dinfo.dct_method = J_DCT_METHOD::JDCT_IFAST;</span><br><span class="line">dinfo.do_fancy_upsampling = <span class="number">0</span>;</span><br><span class="line">dinfo.two_pass_quantize = <span class="number">0</span>;</span><br><span class="line">dinfo.dither_mode = J_DITHER_MODE::JDITHER_ORDERED;</span><br><span class="line">jpeg_start_decompress(&amp;<span class="keyword">mut</span> dinfo);</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut</span> output_image = <span class="built_in">vec!</span>[<span class="number">0</span>; (dinfo.output_width * dinfo.output_height * <span class="number">3</span>) <span class="keyword">as</span> <span class="built_in">usize</span>];</span><br><span class="line"><span class="keyword">let</span> output_buffer = output_image.as_mut_ptr();</span><br><span class="line"><span class="keyword">let</span> row_stride:<span class="built_in">u64</span> = dinfo.output_width <span class="keyword">as</span> <span class="built_in">u64</span> * <span class="number">3</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut</span> buffer = malloc(row_stride <span class="keyword">as</span> <span class="built_in">usize</span>) <span class="keyword">as</span> *<span class="keyword">mut</span> <span class="built_in">u8</span>;</span><br><span class="line"><span class="keyword">while</span> dinfo.output_scanline &lt; dinfo.output_height &#123;</span><br><span class="line">jpeg_read_scanlines(&amp;<span class="keyword">mut</span> dinfo, &amp;<span class="keyword">mut</span> buffer, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">let</span> output_row = &amp;output_buffer.offset((dinfo.output_scanline <span class="keyword">as</span> <span class="built_in">isize</span> - <span class="number">1</span>) * row_stride <span class="keyword">as</span> <span class="built_in">isize</span>);</span><br><span class="line">ptr::copy(buffer, *output_row, row_stride <span class="keyword">as</span> <span class="built_in">usize</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">jpeg_finish_decompress(&amp;<span class="keyword">mut</span> dinfo);</span><br><span class="line">jpeg_destroy_decompress(&amp;<span class="keyword">mut</span> dinfo);</span><br><span class="line"><span class="comment">// fclose(infile);</span></span><br><span class="line">free(buffer <span class="keyword">as</span> *<span class="keyword">mut</span> c_void);</span><br><span class="line">output_image.chunks(<span class="number">3</span>).collect::&lt;<span class="built_in">Vec</span>&lt;_&gt;&gt;().into_iter().flat_map(|pixel| &#123;</span><br><span class="line">                <span class="built_in">vec!</span>[pixel[<span class="number">0</span>], pixel[<span class="number">1</span>], pixel[<span class="number">2</span>], <span class="number">255</span>].into_iter()</span><br><span class="line">            &#125;).collect::&lt;<span class="built_in">Vec</span>&lt;_&gt;&gt;()</span><br><span class="line">&#125;</span><br><span class="line">&#125;)).map_err(|e| &#123;</span><br><span class="line">std::io::Error::new(std::io::ErrorKind::InvalidData, <span class="built_in">format!</span>(<span class="string">"&#123;:?&#125;"</span>, e))</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>However, <code>catch_unwind</code> will try to restore the thread with its stack, which could slow down the program. Also it’s not recommanded for a general try/catch situation. A better way is to use <code>setjmp</code>/<code>longjmp</code> in Rust FFI.</p><h2 id="No-local-jump-in-FFI"><a href="#No-local-jump-in-FFI" class="headerlink" title="No-local jump in FFI"></a>No-local jump in FFI</h2><p>First we should declare extern C binding for the two functions.</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> &#123;</span><br><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">setjmp</span></span>(env: *<span class="keyword">mut</span> c_void) -&gt; c_int;</span><br><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">longjmp</span></span>(env: *<span class="keyword">mut</span> c_void, val: c_int);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>No-local jump needs a buffer to store the stack. Create a struct to store the buffer. We will name it <code>my_error_mgr</code>. To make this easier, the struct is not handling error messages.</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#[allow(non_camel_case_types)]</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">my_error_mgr</span></span> &#123;</span><br><span class="line"><span class="keyword">pub</span> err_mgr: jpeg_error_mgr,</span><br><span class="line"><span class="keyword">pub</span> setjmp_buffer: *<span class="keyword">mut</span> c_void</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>To use the custom error manager, we need to pass <code>my_error_mgr.err_mgr</code> to <code>jpeg_std_err</code> to initialize.</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="keyword">mut</span> setjmp_buffer: [c_int; <span class="number">27</span>] = [<span class="number">0</span>; <span class="number">27</span>];</span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut</span> my_err = my_error_mgr &#123; err_mgr: err, setjmp_buffer: mem::transmute(&amp;<span class="keyword">mut</span> setjmp_buffer) &#125;;</span><br><span class="line">dinfo.common.err = jpeg_std_error(&amp;<span class="keyword">mut</span> my_err.err_mgr);</span><br></pre></td></tr></table></figure><p>Next step is to create a function for error handling, bind it to the <code>err_mgr</code>. The handler will simply jump with value <code>1</code>.</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// declare the function somewhere before</span></span><br><span class="line"><span class="keyword">extern</span> <span class="string">"C"</span> <span class="function"><span class="keyword">fn</span> <span class="title">libjpeg_error_handler</span></span>(c_info: &amp;<span class="keyword">mut</span> jpeg_common_struct) &#123;</span><br><span class="line"><span class="keyword">let</span> my_err: *<span class="keyword">mut</span> my_error_mgr = c_info.err <span class="keyword">as</span> *<span class="keyword">mut</span> my_error_mgr;</span><br><span class="line"><span class="keyword">unsafe</span> &#123; longjmp((*my_err).setjmp_buffer, <span class="number">1</span>); &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">my_err.err_mgr.error_exit = <span class="literal">Some</span>(libjpeg_error_handler);</span><br></pre></td></tr></table></figure><p>we will check <code>setjmp</code> before the decoding process begins, if an expection did occur, return an <code>Err</code> and prevent the crash.</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> setjmp(my_err.setjmp_buffer) != <span class="number">0</span> &#123;</span><br><span class="line">    jpeg_destroy_decompress(&amp;<span class="keyword">mut</span> dinfo);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">Err</span>(std::io::Error::new(std::io::ErrorKind::InvalidData, <span class="string">"jpeg decode error"</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The complete example:</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> <span class="string">"C"</span> <span class="function"><span class="keyword">fn</span> <span class="title">libjpeg_error_handler</span></span>(c_info: &amp;<span class="keyword">mut</span> jpeg_common_struct) &#123;</span><br><span class="line"><span class="keyword">let</span> my_err: *<span class="keyword">mut</span> my_error_mgr = c_info.err <span class="keyword">as</span> *<span class="keyword">mut</span> my_error_mgr;</span><br><span class="line"><span class="keyword">unsafe</span> &#123; longjmp((*my_err).setjmp_buffer, <span class="number">1</span>); &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#[allow(non_camel_case_types)]</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">my_error_mgr</span></span> &#123;</span><br><span class="line"><span class="keyword">pub</span> err_mgr: jpeg_error_mgr,</span><br><span class="line"><span class="keyword">pub</span> setjmp_buffer: *<span class="keyword">mut</span> c_void</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">extern</span> &#123;</span><br><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">setjmp</span></span>(env: *<span class="keyword">mut</span> c_void) -&gt; c_int;</span><br><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">longjmp</span></span>(env: *<span class="keyword">mut</span> c_void, val: c_int);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">read_jpeg</span></span>(input_buffer: &amp;[<span class="built_in">u8</span>], target_width: <span class="built_in">u32</span>, target_height: <span class="built_in">u32</span>) -&gt; stdio::<span class="built_in">Result</span>&lt;<span class="built_in">Vec</span>&lt;<span class="built_in">u8</span>&gt;&gt; &#123;</span><br><span class="line"><span class="keyword">unsafe</span> &#123;</span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut</span> dinfo: jpeg_decompress_struct = mem::zeroed();</span><br><span class="line"><span class="keyword">let</span> size = mem::size_of_val(&amp;dinfo) <span class="keyword">as</span> size_t;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut</span> err: jpeg_error_mgr = mem::zeroed();</span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut</span> setjmp_buffer: [c_int; <span class="number">27</span>] = [<span class="number">0</span>; <span class="number">27</span>];</span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut</span> my_err = my_error_mgr &#123; err_mgr: err, setjmp_buffer: mem::transmute(&amp;<span class="keyword">mut</span> setjmp_buffer) &#125;;</span><br><span class="line">dinfo.common.err = jpeg_std_error(&amp;<span class="keyword">mut</span> my_err.err_mgr);</span><br><span class="line">my_err.err_mgr.error_exit = <span class="literal">Some</span>(libjpeg_error_handler);</span><br><span class="line"><span class="keyword">if</span> setjmp(my_err.setjmp_buffer) != <span class="number">0</span> &#123;</span><br><span class="line">jpeg_destroy_decompress(&amp;<span class="keyword">mut</span> dinfo);</span><br><span class="line"><span class="keyword">return</span> <span class="literal">Err</span>(stdio::Error::new(stdio::ErrorKind::InvalidData, <span class="string">"jpeg decode error"</span>));</span><br><span class="line">&#125;</span><br><span class="line">jpeg_CreateDecompress(&amp;<span class="keyword">mut</span> dinfo, JPEG_LIB_VERSION, size);</span><br><span class="line">jpeg_mem_src(&amp;<span class="keyword">mut</span> dinfo, input_buffer.as_ptr(), input_buffer.len() <span class="keyword">as</span> <span class="built_in">u64</span>);</span><br><span class="line">jpeg_read_header(&amp;<span class="keyword">mut</span> dinfo, <span class="literal">true</span> <span class="keyword">as</span> <span class="built_in">i32</span>);</span><br><span class="line"><span class="keyword">let</span> image_width = dinfo.image_width;</span><br><span class="line"><span class="keyword">let</span> image_height = dinfo.image_height;</span><br><span class="line"><span class="keyword">if</span> target_width != image_width || target_height != image_height &#123;</span><br><span class="line"><span class="keyword">let</span> (scale, scale_denom) = select_decompress_idct_factor(image_width, image_height, target_width, target_height);</span><br><span class="line">dinfo.scale_num = scale;</span><br><span class="line">dinfo.scale_denom = scale_denom;</span><br><span class="line">&#125;</span><br><span class="line">dinfo.dct_method = J_DCT_METHOD::JDCT_IFAST;</span><br><span class="line">dinfo.do_fancy_upsampling = <span class="number">0</span>;</span><br><span class="line">dinfo.two_pass_quantize = <span class="number">0</span>;</span><br><span class="line">dinfo.dither_mode = J_DITHER_MODE::JDITHER_ORDERED;</span><br><span class="line">jpeg_start_decompress(&amp;<span class="keyword">mut</span> dinfo);</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut</span> output_image = <span class="built_in">vec!</span>[<span class="number">0</span>; (dinfo.output_width * dinfo.output_height * <span class="number">3</span>) <span class="keyword">as</span> <span class="built_in">usize</span>];</span><br><span class="line"><span class="keyword">let</span> output_buffer = output_image.as_mut_ptr();</span><br><span class="line"><span class="keyword">let</span> row_stride:<span class="built_in">u64</span> = dinfo.output_width <span class="keyword">as</span> <span class="built_in">u64</span> * <span class="number">3</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut</span> buffer = malloc(row_stride <span class="keyword">as</span> <span class="built_in">usize</span>) <span class="keyword">as</span> *<span class="keyword">mut</span> <span class="built_in">u8</span>;</span><br><span class="line"><span class="keyword">while</span> dinfo.output_scanline &lt; dinfo.output_height &#123;</span><br><span class="line">jpeg_read_scanlines(&amp;<span class="keyword">mut</span> dinfo, &amp;<span class="keyword">mut</span> buffer, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">let</span> output_row = &amp;output_buffer.offset((dinfo.output_scanline <span class="keyword">as</span> <span class="built_in">isize</span> - <span class="number">1</span>) * row_stride <span class="keyword">as</span> <span class="built_in">isize</span>);</span><br><span class="line">ptr::copy(buffer, *output_row, row_stride <span class="keyword">as</span> <span class="built_in">usize</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">jpeg_finish_decompress(&amp;<span class="keyword">mut</span> dinfo);</span><br><span class="line">jpeg_destroy_decompress(&amp;<span class="keyword">mut</span> dinfo);</span><br><span class="line"><span class="comment">// fclose(infile);</span></span><br><span class="line">free(buffer <span class="keyword">as</span> *<span class="keyword">mut</span> c_void);</span><br><span class="line"><span class="keyword">let</span> alpha = &amp;[<span class="number">255</span>];</span><br><span class="line"><span class="keyword">let</span> output_image = output_image.chunks(<span class="number">3</span>).flat_map(|chunk| &#123;</span><br><span class="line">chunk.into_iter().chain(alpha)</span><br><span class="line">&#125;).map(|i| *i).collect::&lt;<span class="built_in">Vec</span>&lt;_&gt;&gt;();</span><br><span class="line">        <span class="literal">Ok</span>(output_image)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;code&gt;setjmp&lt;/code&gt;/&lt;code&gt;longjmp&lt;/code&gt; is used for &lt;a href=&quot;https://en.wikipedia.org/wiki/Setjmp.h&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;no-l
      
    
    </summary>
    
    
      <category term="Rust" scheme="http://daiheitan.github.io/blog/tags/Rust/"/>
    
  </entry>
  
  <entry>
    <title>Rust futures at a glance</title>
    <link href="http://daiheitan.github.io/blog/2016/12/07/Rust-futures-at-a-glance/"/>
    <id>http://daiheitan.github.io/blog/2016/12/07/Rust-futures-at-a-glance/</id>
    <published>2016-12-07T07:49:59.000Z</published>
    <updated>2019-04-25T09:21:38.937Z</updated>
    
    <content type="html"><![CDATA[<p>Recently I am working on a project involving a server which is both CPU and I/O heavy.As a developer with a Node.js background, I realized that Node.js is not doing well in a CPU-heavy situation. And that’s why I switched to Rust. I had no doubt about Rust’s well played lifetime &amp; borrow design and the ability to easily ship parallel code to accomplish CPU-heavy jobs. But it’s the I/O handling that remains a problem.</p><p>Rust’s standard library has a <a href="https://doc.rust-lang.org/std/net/struct.TcpStream.html" target="_blank" rel="noopener">TcpStream</a> which is sync. It’s known that sync I/O operations could be a huge drawback of program performace as the thread has to wait for I/O to finish. Currently guys in the Rust community are working hard on a solution of this based on <a href="https://github.com/carllerche/mio" target="_blank" rel="noopener">mio</a>.</p><p>Before all of this, here’s some background knowledge about async I/O.</p><h2 id="Async-IO"><a href="#Async-IO" class="headerlink" title="Async IO"></a>Async IO</h2><p>There’re two async IO models: the completion model and the readiness model.</p><p>The completion model is quite straightforward. The program provides a buffer to the kernel and schedules a <em>callback</em>. The kernel will eventually fill the buffer with async-got data and call the callback with the data.</p><p>The problem of this model is that it involves many allocations, also quickly becomes too complicated especially when composing async jobs. The situation is quite similar to Node.js’s <a href="http://stackabuse.com/avoiding-callback-hell-in-node-js/" target="_blank" rel="noopener">callback hell</a>.</p><p>The readiness model is more passive. The program polls a socket, and gets an <code>EWOULDBLOCK</code> error if it is not ready. The kernel(<code>epoll</code>, in fact) will register the program for further notification once the socket is ready, and <em>waits</em> for state changes of the socket. When notified, the program could poll again and get the data.</p><p>So in the readiness model, an <em>event loop</em> must be introduced to continuously call <code>epoll_wait</code> to get socket fd changes. <code>Mio</code> is such a library in Rust.</p><blockquote><p>Note that even though Node.js’s standard APIs are in a callback favor, it is <strong>NOT</strong> using the completion model. Node.js is based on <code>libuv</code>, which is a successor of <code>libev</code>, the well-known event loop implementation</p></blockquote><p>Further readings for this section:</p><ol><li><a href="http://hermanradtke.com/2015/07/12/my-basic-understanding-of-mio-and-async-io.html" target="_blank" rel="noopener">My Basic Understanding of mio and Asynchronous IO</a></li><li><a href="http://man7.org/linux/man-pages/man7/epoll.7.html" target="_blank" rel="noopener">Epoll</a></li></ol><h2 id="Futures-in-Rust"><a href="#Futures-in-Rust" class="headerlink" title="Futures in Rust"></a>Futures in Rust</h2><p>So assume that we already have the low-level event-loop layer. How to define and construct an async task? In Node.js we have <em>Promise</em>. A <em>promise</em> is a special object with a <code>.then()</code> API. The promise model has two major drawbacks:</p><p>It could not be scheduled. The promise is sent to the event loop and executed as soon as created. And it could not be canceled.</p><p><a href="http://aturon.github.io/blog/" target="_blank" rel="noopener">Aturon</a> takes a clever design named as <a href="https://github.com/alexcrichton/futures-rs/" target="_blank" rel="noopener">futures-rs</a> to solve the problems. The <code>Future</code> trait is like this:</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Future</span></span> &#123;</span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">Item</span></span>;</span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">Error</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">fn</span> <span class="title">poll</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>) -&gt; <span class="built_in">Result</span>&lt;Async&lt;Self::Item&gt;, Self::Error&gt;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>A <em>Future</em> acts as a node in a large state machine. Each time polled, the <code>.poll()</code> API tells which state to go to. If the future is ready to be resolved, it returns <code>Async::Ready(data)</code> with the <em>data</em> resolved. If not, <code>Async::NotReady</code> is returned to indicate that it should be polled later.</p><p>To cancel a future, an event loop could simply stop polling the future any more, which could be achieved by simply <code>drop</code> the future instance. Also as a future is a state machine, it will not move to any state without polling.</p><blockquote><p>Note that <code>futures-rs</code> is an abstract layer of design pattern. It is not bound to any real async library (like mio) or event loop implementation(like tokio-core).</p></blockquote><h2 id="Run-a-Future"><a href="#Run-a-Future" class="headerlink" title="Run a Future"></a>Run a Future</h2><p>We now have a Future instance, but who should poll the future? <code>Task</code> is introduced as the execution unit of a future. the <code>.wait</code> (blocking current thread) and <code>.spawn</code> (dispatch the future to a thread) APIs are used to create tasks to execute the future passed in.</p><p>When a future returns <code>NotReady</code>, the task is halted to wait for some event to resume. When running many <em>tasks</em>, you have to decide which task to run on which worker thread. When an event occurs, you have to decide which task to wake up. So clearly there’s need for another layer to handle all of this, connect the futures model to the actual async world. This is where <a href="https://github.com/tokio-rs/tokio-core" target="_blank" rel="noopener">tokio-core</a> kicks in. Tokio-core acts as an event loop (based on mio’s event loop) to poll <code>fd</code>s, get the events and schedule tasks.</p><p>So now let’s go back to the task. When a future is <code>NotReady</code>, the task halts. How?</p><p>There is an API <a href="https://docs.rs/futures/0.1.6/futures/task/fn.park.html" target="_blank" rel="noopener">.park()</a>, which should be called in a future’s <code>.poll()</code> function. It acts like <code>thread::park</code> and halts the task at the point called. This API returns a <em>handle</em> to the caller to resume the task at a proper time later. A halted task will not block current thread (aka the tokio-core event loop), which allows the event loop to schedule another task to keep it busy.</p><p>Further readings for this section:</p><ul><li><a href="http://aturon.github.io/blog/2016/09/07/futures-design/" target="_blank" rel="noopener">Design futures for Rust</a></li><li><a href="http://aturon.github.io/blog/2016/08/11/futures/" target="_blank" rel="noopener">Zero cost futures in Rust</a></li></ul><h2 id="Custom-Future-Myth"><a href="#Custom-Future-Myth" class="headerlink" title="Custom Future Myth"></a>Custom Future Myth</h2><p>Say we have a <code>TcpStream</code> and want to read 10 lines from it. It’s easy to come up with a custom Future like <code>TenTimesReadFuture</code>.</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TenTimesReadFuture</span></span> &#123;</span><br><span class="line">  buffer: <span class="built_in">Vec</span>&lt;<span class="built_in">u8</span>&gt;,</span><br><span class="line">  line_count: <span class="built_in">u8</span>,</span><br><span class="line">  stream: TcpStream</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">impl</span> Future <span class="keyword">for</span> TenTimesReadFuture &#123;</span><br><span class="line">  <span class="comment">// omitted</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In the <code>.poll()</code> method, read a line from the stream and store it to <code>buffer</code> and check the <code>line_count</code>. If it reaches 10 lines, resolve the future with the buffer.</p><p>But this will not work and <code>.poll()</code> will be called only once. That’s because behind tokio-core, <code>mio</code>‘s event loop is using the <em>edge triggering model</em>. In the event loop, when a resource’s ready, the registered callback (the task) is only notified <em>once</em>. So you have to process all the bytes once the socket is ready and <code>.poll()</code> is called.</p><p>If you do need to wait for something like operations in a worker thread, you have to use the <code>park/unpark</code> API to pause the poll and resume it later on.</p><h2 id="Tokio-core"><a href="#Tokio-core" class="headerlink" title="Tokio-core"></a>Tokio-core</h2><p>Till now it’s quite clear that the event loop is quite important for all of this to actually work. Tokio-core is such a crate.</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> core = tokio_core::reactor::Core::new();</span><br><span class="line"><span class="keyword">let</span> lp = core.handle();</span><br><span class="line"><span class="comment">// move the handle around</span></span><br><span class="line">core.run(a_future).unwrap();</span><br></pre></td></tr></table></figure><p><code>.run</code> will <strong>block</strong> current thread to poll a future to end. It will be super inconvenient to call this API every time. A proper way is to obtain a <code>Handle</code> of the loop via <code>core.handle()</code>. You could move the handle around, cheap clone it to various structs. Later on, call <code>handle.spawn()</code> to spawn a future to the event loop.</p><p>Even though Futures-rs is ready for multi-thread use, tokio’s core is single-threaded. It’s clearer to keep the event loop on a single thread and leave the multi-thread thing to the user. There’s <code>futures-cpupool</code> to spread cpu-heavy jobs to a thread pool. You could also use <code>core.remote()</code> to get a remote handle, which is <code>Send</code> and could be used in another thread.</p><p>Normally in a multi-threaded server, it’s better to spawn several threads and run an event loop on each. You could reuse the TCP port through the threads. It allows the kernel to dispatch different incoming sockets among the worker threads.</p><p>Tokio-core wraps mio’s APIs and serves the <code>net</code> module with async TCP/UDP bindings. In the 0.1.0 release, there’s <code>io</code> module with some convenient methods for manipulating the streams.</p><p>For most users, working directly with tokio-core is still too low-level. So there’s another abstraction layer to sculpture a common workflow. It is <a href="https://github.com/tokio-rs/tokio-proto" target="_blank" rel="noopener">Tokio-proto</a> which is usually used together with <a href="https://github.com/tokio-rs/tokio-service" target="_blank" rel="noopener">Tokio-service</a>. But that’s too much for this article.</p><p>For HTTP users, hyper’s tokio branch is based on tokio-proto/futures stack, so you have to wait for some time until everything settled down.</p><h2 id="Futures-in-practice-How-to-return-a-future"><a href="#Futures-in-practice-How-to-return-a-future" class="headerlink" title="Futures in practice: How to return a future?"></a>Futures in practice: How to return a future?</h2><p>Now it’s time to put all of this stuff into practice. When actually using futures to write a project, a critical problem will soon hit you: What’s the correct way to return a <code>Future</code>?</p><p>Aturon has a nice <a href="https://github.com/alexcrichton/futures-rs/blob/master/TUTORIAL.md" target="_blank" rel="noopener">tutorial</a> about the baby steps of using futures. I urge everyone to read it before using the futures pattern. In the tutorial, he mentions that there are several ways to return a future:</p><ul><li>Box wrapping the future</li><li>Use a custom struct to wrap the future</li><li>Directly return the type</li><li>return <code>impl trait</code></li></ul><p>In my opinion, the forth option is the one and only elegant way to go. Directly returning the type is nearly impossible when you are chaning futures (that’s when the type could be really <em>HUGE</em>). Using a custom struct also requires you to declare the type of the future in the struct. So the same problem occurs again.</p><p>Box wrapping seems to be a good approach. In fact it’s really easy for a newbie like me to use the <code>.boxed()</code> API on futures and return <code>BoxFuture</code> here and there. But given a second thought, the API should <em>not</em> be used. <code>.boxed()</code> requires a <code>Send + &#39;static</code> bound, which is not needed in most cases, especially when most projects are using <code>tokio-core</code> which is single-threaded. If you do need to return a boxed future, it’d better to create your own boxed type without the <code>Send</code> bound.</p><p>That’s not the biggest problem, however. A <code>Box</code> in rust always means runtime overhead. THe compiler could not know the type statically. In fact in this scenario we DO know the returned type, just not bothered to write it down.</p><p>Use the latest <a href="https://github.com/rust-lang/rfcs/blob/master/text/1522-conservative-impl-trait.md" target="_blank" rel="noopener">impl trait</a> signature seems to be the best way. It is super clear and still is statically analyzed down to a specific type. Just return <code>impl Future&lt;Item=T, Error=E&gt;</code> and you are good to go.</p><blockquote><p>A common myth is that <code>impl trait</code> is similar to that it is in OO languages. In fact it is <strong>NOT</strong>. You could not return any of the trait implementations, only <strong>ONE</strong> of them. It’s just a grammar sugar, and that’s the base of why it do not aquire any runtime overhead.</p></blockquote><p><strong>Note A:</strong></p><p>A common scenario is that you need to return one of several futures based on a condition.</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> condition &#123;</span><br><span class="line">  future_a</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  future_b</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>It’s not so obvious that you could always chain futures to combine these futures into one.</p><p>An enum type <code>Either</code> is <a href="https://github.com/alexcrichton/futures-rs/pull/271" target="_blank" rel="noopener">merged</a> and landed in futures-rs 0.1.7 to solve this problem.</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> condition &#123;</span><br><span class="line">  Either::A(a)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    Either::B(b)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>And now you could return the one and only Future implementation without breaking the rules.</p><p><strong>Note B:</strong></p><p>When using <code>impl Future</code>, a common situation is that the compiler would complain about “only named lifetimes are allowed in impl trait”. This is to be because of your method has an anoymouse lifetime. Use the following pattern:</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">some_method</span></span>&lt;<span class="symbol">'a</span>&gt;(arg: SomeArgument) -&gt; <span class="keyword">impl</span> Future&lt;Item=(), Error=()&gt; + <span class="symbol">'a</span> &#123;</span><br><span class="line">  <span class="comment">// omitted</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Mio and Futures is the rusty answer of the async problem. It is carefully crafted and come with a elegant design. Though it could be hard to learn about the concepts and the patterns, it is really a neat piece of library/stack to use.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Recently I am working on a project involving a server which is both CPU and I/O heavy.As a developer with a Node.js background, I realize
      
    
    </summary>
    
    
      <category term="Rust" scheme="http://daiheitan.github.io/blog/tags/Rust/"/>
    
  </entry>
  
</feed>
