<!DOCTYPE html>
<html>
  <head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
    <title>Zimon Dai&#39;s Random Thoughts</title>
  </head>
  <body>
    <div class="container">
      <section id="main">
  <div class="page-title">Data race, atomic and reordering</div>
  <header>Posted at 2017-05-19, tagged as: 
    
      <a class="article-tag-link-link" href="/blog/tags/Atomic/">Atomic</a>, <a class="article-tag-link-link" href="/blog/tags/Concurrency/">Concurrency</a>, <a class="article-tag-link-link" href="/blog/tags/Memory/">Memory</a>
    .
    <a href="/blog">Go Back</a>
  </header>
  <div class="post-detail">
    <p>Everything is much more complicated in a multithreaded world. Everyone knows about data races, while few knows why and how to effeciently avoid that.</p>
<p>Data race occurs when multiple threads accessing the same memory location at the same time (Read &amp; Write). If one thread writes to the memory, another thread is not updated and uses the old value to do its own calculcation, undefined behaviour happens. Data race should not be a problem is any interactions made to a memory location is well defined.</p>
<p>By using the word <em>interaction</em>, we mean <em>Read</em> and <em>Write</em>, or in the memory model terminology, <em>Load</em> and <em>Store</em>.</p>
<p>By using the word <em>well defined</em>, we mean the interactions for one memory model in one thread do not collision with other threads in this time period.</p>
<p>This post introduces memory reordering, how it affects multithreaded program behaviour, why does it happen and what we could do to avoid data race by avoiding it.</p>
<h1 id="Memory-Reordering"><a href="#Memory-Reordering" class="headerlink" title="Memory Reordering"></a>Memory Reordering</h1><p>Memory reordering is quite interesting. If writing single threaded program, you may never noticed this. As the key principle of memory reordering is keep single-threaded behaviour not changed. Reordering includes compiler and runtime reordering.</p>
<h2 id="Compile-time-reordering"><a href="#Compile-time-reordering" class="headerlink" title="Compile time reordering"></a><a href="http://preshing.com/20120625/memory-ordering-at-compile-time/" target="_blank" rel="noopener">Compile time reordering</a></h2><p>As the title says, this happens when the compiler thransforming your code into assembly code. The compiler trys to gather all IO to a memory location into one block to optimise memory usage.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> A, B;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    A = B + <span class="number">1</span>;</span><br><span class="line">    B = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>will be translated to:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">mov     eax, DWORD PTR B</span><br><span class="line">mov     DWORD PTR B, 0</span><br><span class="line">add     eax, 1</span><br><span class="line">mov     DWORD PTR A, eax</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>The <em>save</em> to <code>B</code> is lifted before the <em>save</em> to <code>A</code>, so I/O to <code>B</code> happens together. As mentioned before, this will not affect the runtime in a single-threaded program. But when it comes to multithreaded, the program may run not like what you expected.</p>
<p>To avoid compile time reordering, one could add compiler instructions to tell the compiler that reordering should not be done at certain position. In C++11, you could use <code>asm volatile(&quot;&quot; ::: &quot;memory&quot;)</code>. In rust you could use <code>asm!(&quot;&quot; ::: &quot;memory&quot; : &quot;volatile&quot;)</code>.</p>
<h2 id="Runtime-reordering"><a href="#Runtime-reordering" class="headerlink" title="Runtime reordering"></a>Runtime reordering</h2><p>This is the interecting part. The processor could do runtime reordering when executing your program. That’s because every process has a inline-cache (like 32KB size) which supported I/O in 1 or 2 cycles. But a trip to the memory could cost hundred of cycles. So the processor would like to cache as much as he could and visit the memory as less. Not only the read is a problem, when some data is written, it first comes to the L1 cache, then got flushed to the memroy and L2 cache when the L1 cache is full. We could say that the time point of I/O is totally undefined.</p>
<p>Let’s say we have 4 threads A, B, C and D. A and B both read then write to <code>variable</code>, C and D are <em>observers</em>. Despite of reordering that could occur on every thread, C and D may end with totally different result of IO observation. C may see Load-A -&gt; Load-B -&gt; Store-A -&gt; Store-B, whilc D may see Load-A -&gt; Store-A -&gt; Load-B -&gt; Store-B.</p>
<h3 id="Sequential-Consistency"><a href="#Sequential-Consistency" class="headerlink" title="Sequential Consistency"></a>Sequential Consistency</h3><p>If a system could keep the observation result same across every thread, we say this system has <em>Sequential Consistency</em>. In a sequential consist system, all I/O are in order, no extra worry.</p>
<h3 id="Relaxed-consistency"><a href="#Relaxed-consistency" class="headerlink" title="Relaxed consistency"></a>Relaxed consistency</h3><p>Note that the most popular system X86 (intel, AMD) is not sequential consist.This is the major problem when writing multithreaded programs. And that’s where memory barriers come in. The creator of java compiler, Doug Lea has a <a href="http://g.oswego.edu/dl/jmm/cookbook.html" target="_blank" rel="noopener">famous article</a> about compilers, in which he defined 4 types of memory barriers: LoadLoad, LoadStore, StoreStore, StoreLoad. E.g, <code>LoadStore</code> is to make a barrier between a <code>Load</code> and a following <code>Store</code>, to prevent the <code>store</code> to be reordered to happen before the <code>load</code>.</p>
<p>Relaxed consitency means that some reodering are allowed if they do not across the mentioned four types of barriers.</p>
<p>Major ref of the following is <a href="http://preshing.com/20120710/memory-barriers-are-like-source-control-operations/" target="_blank" rel="noopener">this</a></p>
<h4 id="LoadLoad"><a href="#LoadLoad" class="headerlink" title="LoadLoad"></a><code>LoadLoad</code></h4><p><code>LoadLoad</code> acts quite like a <code>git pull</code>. It meas that <em>a read A could only happens when a former read B happens</em>. Note that the “former B” read may or may not be the latest value of B, this read and the former read may even not be the same variable / memory location. It’s quite useful even though looks weak at the first glance.</p>
<p>Say we have a variable which is manipulated by several threads. We could create a second shared boolean variable <code>isChanged</code>, if <code>isChanged</code> is set to <code>true</code>, we know the variable is changed by another thread. If we read the variable <strong>at this time point</strong>, the value should be updated.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (isUpdated) &#123;</span><br><span class="line">    LOADLOAD_FENCE();</span><br><span class="line">    <span class="keyword">return</span> Value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The fact that <code>isUpdated</code> is the latest value or not is really not important. We now have a defined bound to between <code>isUpdated</code> and <code>Value</code>, that’s enough. This pattern is a basis to the widely used double check lock pattern.</p>
<h4 id="StoreStore"><a href="#StoreStore" class="headerlink" title="StoreStore"></a><code>StoreStore</code></h4><p>It acts quites like a <code>git push</code>. Using the previous example, a <code>StoreStore</code> barrier is useful when we need to update <code>Value</code></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Value = x;</span><br><span class="line">STORESTORE_FENCE();</span><br><span class="line">isUpdated = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>This ensures <code>isUpdated</code> is set <strong>after</strong> Value, and tightly bind the two stores.</p>
<h4 id="LoadStore"><a href="#LoadStore" class="headerlink" title="LoadStore"></a><code>LoadStore</code></h4><p>LoadStore barriers are needed only on those out-of-order procesors in which waiting store instructions can bypass loads.</p>
<h4 id="StoreLoad"><a href="#StoreLoad" class="headerlink" title="StoreLoad"></a><code>StoreLoad</code></h4><p><code>StoreLoad</code> is quite special here. It’s a strong memory barrier and more expensive, which ensures that <strong>ALL</strong> stores performed before the barrier are visible to other processors.</p>
<p>Note that this is not equal to a <code>LoadLoad</code> and a <code>StoreStore</code>. As <code>LoadLoad</code> may not get the latest version between all processors for you, but a <code>StoreLoad</code> makes sure of that.</p>
<h1 id="Prevent-data-race-with-a-lock"><a href="#Prevent-data-race-with-a-lock" class="headerlink" title="Prevent data race with a lock"></a>Prevent data race with a lock</h1><h2 id="Lock-basics"><a href="#Lock-basics" class="headerlink" title="Lock basics"></a>Lock basics</h2><p><a href="https://en.wikipedia.org/wiki/Lock_(computer_science" target="_blank" rel="noopener">Lock</a>) is designed to enforce mutual exclusion concurrency control. And it’s sync. You could prevent data race by acquire a lock, make your updates inside the scope, and finally release the lock.</p>
<p>Usually when we say <em>lock</em>, it refers to a <em>Mutex</em> lock(readwrite lock), which a lock must be held both reading and writing. There’s also RwLock allowing multiple readers <strong>or</strong> one writer. But in fact on many OS, rwlocks are just mutex locks underneath.</p>
<h3 id="Double-check-lock"><a href="#Double-check-lock" class="headerlink" title="Double check lock"></a>Double check lock</h3><p>Mutex lock is easy to understand and use, but indeed not so efficient when thought through. There’s double-check lock which works with memory barriers, explained <a href="http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/" target="_blank" rel="noopener">here</a></p>
<h1 id="Prevent-data-race-with-memory-fences"><a href="#Prevent-data-race-with-memory-fences" class="headerlink" title="Prevent data race with memory fences"></a>Prevent data race with memory fences</h1><p>If you <a href="http://preshing.com/20120612/an-introduction-to-lock-free-programming/" target="_blank" rel="noopener">do not want to use</a> locks, and hates atomics because of lacking of scale and performace in certain conditions, the choice left is to dive into the dirty ground of memory barriers.</p>
<h2 id="Memory-fences-Acquire-and-release"><a href="#Memory-fences-Acquire-and-release" class="headerlink" title="Memory fences: Acquire and release"></a>Memory fences: Acquire and release</h2><p>C++ 11 introduces low-level lock-free operations: acquire and release.</p>
<p>An <em>acquire fence</em> prevents memory reordering after a read. Which equals to a LoadLoad + LoadStore.</p>
<p>A <em>release fence</em> prevents memory reordering of write after read/write, equals to LoadStore + StoreStore</p>
<p>A well-told article <a href="http://preshing.com/20130922/acquire-and-release-fences/" target="_blank" rel="noopener">here</a>.</p>
<h1 id="Prevent-data-race-with-atomics"><a href="#Prevent-data-race-with-atomics" class="headerlink" title="Prevent data race with atomics"></a>Prevent data race with atomics</h1><p>Locks are heavy and inefficient. Luckily they are not the only option to avoid data races in multithreaded programming. As described in previous sections, if we could instruct the processors to load/store data in a proper way, data race could totally be avoided. This is lock-free programming.</p>
<h2 id="Force-SC-Java-volatile-and-C-Atomic"><a href="#Force-SC-Java-volatile-and-C-Atomic" class="headerlink" title="Force SC: Java volatile and C++ Atomic"></a>Force SC: Java volatile and C++ Atomic</h2><p>As mentioned before, in sequential consistency, IO are in-order. So if we could achieve SC, problem solved! This is how Java’s <code>volatile</code> is <a href="https://bartoszmilewski.com/2008/11/11/who-ordered-sequential-consistency/" target="_blank" rel="noopener">done</a> (also C++11’s atomic).</p>
<ul>
<li>Issue a StoreStore barrier before each volatile store.</li>
<li>Issue a StoreLoad barrier after each volatile store.</li>
<li>Issue LoadLoad and LoadStore barriers after each volatile load.</li>
</ul>
<p>After that, Java compiler will use data analysis to remove some of the locks. And if it’s X86 system, LoadLoad and StoreStore locks are translated to no-ops. That’s <a href="https://bartoszmilewski.com/2008/11/05/who-ordered-memory-fences-on-an-x86/" target="_blank" rel="noopener">because of</a> the special structure of X86. So there would be generally to say, only the <code>StoreLoad</code> barrier which is still quite heavy.</p>
<p>Atomics are really easy to use, with the <code>.load()</code> and <code>.store()</code> API we mentioned before. But the problem is only super basic data structures (like int32, boolean) has atomic types. Many widely used data structures do not have a standard atomic type.</p>
<p>Note that in C++ 11, Atomics are <strong>defaultly</strong> SC, you could still specify the memory ordering for every <code>load</code> and <code>store</code>. It’s explained <a href="https://bartoszmilewski.com/2008/12/01/c-atomics-and-memory-ordering/" target="_blank" rel="noopener">here</a></p>
<h2 id="Write-your-own-Atomic-type"><a href="#Write-your-own-Atomic-type" class="headerlink" title="Write your own Atomic type"></a>Write your own Atomic type</h2><p><code>load</code> and <code>store</code> are one-dimensional operations. It is not really enough for lock-free operations. Take the add operation as an example, you need to first <em>load</em> the value, perform the addition, and then <em>store</em> the new value back. However, all the different kinds of atomic operations could be built with one essential operation: Compare-and-Swap (CAS). C++11 defines CAS as:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shared.compare_exchange_weak(T&amp; oldValue, T newValue, ...);</span><br></pre></td></tr></table></figure>
<p>This API updates an atomic <code>shared</code> with <code>newValue</code>, only if current value stored in <code>shared</code> equals to <code>oldValue</code>, or it will not perform the update, instead just pull the stored value into <code>oldValue</code>.</p>
<p>A spinlock is easy to come up with:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">uint32_t</span> fetch_multiply(<span class="built_in">std</span>::atomic&lt;<span class="keyword">uint32_t</span>&gt;&amp; shared, <span class="keyword">uint32_t</span> multiplier)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">uint32_t</span> oldValue = shared.load();</span><br><span class="line">    <span class="keyword">while</span> (!shared.compare_exchange_weak(oldValue, oldValue * multiplier)) &#123; &#125;</span><br><span class="line">    <span class="keyword">return</span> oldValue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>By using this, you could write any Read-Modify-Write operations for custom atomic types.</p>

  </div>
</section>
    </div>
    <footer class="footer">
  <div class="theme-changer">
    You are currently using the <span id="theme-indicator">light</span> theme, <a id="theme-change-button">[change]</a>. The color scheme is from the <a href="https://github.com/dracula/dracula-theme" target="_blank">[dracula theme]</a>
    <p>Suscribe this blog, click <a href="/blog/atom.xml">here</a></p>
  </div>
  <link rel="stylesheet" href="/blog/css/style.css">
  <script src="/blog/js/script.js"></script>
  <link rel="stylesheet" href="/blog/css/fira.css">
</footer>
  </body>
</html>